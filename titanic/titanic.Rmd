---
title: "machine learning with titanic dataset"
output: html_document
---

We will compare the performance of different machine learning algorithms on predicting titanic survival

Sections to cover:

 * one-hot encoding
 * model comparison (RF, SVM, NB, LDA, KNN)
 * other models (neural network, xgboost)
 * feature selection
 * hyperparameter tuning
 * autoML
 

```{r setup, include=FALSE,warning = FALSE, error = FALSE, message=FALSE}
library(readr) # CSV file I/O, e.g. the read_csv function
library(reshape2)
library(tidyr)
library(dplyr)
#ML
#library(rattle)
library(caret) 
library(caretEnsemble)
library(xgboost)
library(neuralnet)
library(h2o)
h2o.init()
library(ggplot2) # Data visualization
```

```{r warning = FALSE, error = FALSE, message=FALSE}
train <- read_csv("train.csv")
test <- read_csv("test.csv")

train <- train %>% 
  dplyr::select(-c(PassengerId,Ticket,Cabin, Name)) %>% 
  mutate(Pclass=factor(Pclass))

test <- test %>% 
  dplyr::select(-c(PassengerId,Ticket,Cabin, Name)) %>% 
  mutate(Pclass=factor(Pclass))
```


#Exploration

```{r warning = FALSE, error = FALSE, message=FALSE}
library(GGally)
ggpairs(train, aes(colour = Survived, alpha = 0.4))
```

```{r}
#NA in each col
train%>%summarise_each(funs(sum(is.na(.))))
```

#Imputation
```{r imputation}
train[is.na(train$Age), "Age"] <- median(train$Age, na.rm=T)
test[is.na(test$Age), "Age"] <- median(test$Age, na.rm=T)
test[is.na(test$Fare), "Fare"] <- median(test$Fare, na.rm=T)
train[is.na(train$Embarked), "Embarked"] <- "S"
```

#One-hot encoding
```{r one_hot_encoding}

train_dmy <- dummyVars("~.", data=train, fullRank=T) #generate n-1 columns
train_transform <- data.frame(predict(train_dmy, newdata = train))
train_transform <- train_transform %>% mutate(Survived = factor(Survived))
```

#Split training data
```{r train_test_split}
trainIndex <- createDataPartition(train_transform$Survived, p = .7, list = FALSE)
train_df <- train_transform[trainIndex,]
validate_df <- train_transform[-trainIndex,]
```

#cross validation
```{r}
set.seed(111)
control <- trainControl(method="cv", number=10, verboseIter = TRUE)
```

#modeling in caret
```{r warning = FALSE, error = FALSE, message=FALSE}

model_tree <- train(Survived ~.,
                  method="rpart",
                  train_df,
                  metric = "Accuracy",
                  trControl = control,
                  preProcess = c("center", "scale"))

model_rf <- train(Survived ~.,  
                  method="ranger",
                  train_df, 
                  metric = "roc",
                  tuneLength = 10,
                  trControl = control,
                  preProcess = c("center", "scale"))

model_knn <- train(Survived ~.,  
                  method="knn",
                  train_df, 
                  metric = "roc",
                  trControl = control,
                  preProcess = c("center", "scale"))

model_svclinear <- train(Survived ~.,  
                  method="svmLinear",
                  train_df, 
                  metric = "roc",
                  trControl = control,
                  preProcess = c("center", "scale"))

model_svcradial <- train(Survived ~.,  
                  method="svmRadial",
                  train_df, 
                  metric = "roc",
                  trControl = control,
                  preProcess = c("center", "scale"))

model_lda <- train(Survived ~.,  
                  method="lda",
                  train_df, 
                  metric = "roc",
                  tuneLength = 10,
                  trControl = control,
                  preProcess = c("center", "scale"))

model_nb <- train(Survived ~.,  
                  method="naive_bayes",
                  train_df, 
                  metric = "roc",
                  tuneLength = 10,
                  trControl = control,
                  preProcess = c("center", "scale"))
```

#model comparison
```{r}
model_list <- list(rf = model_rf, 
                   knn = model_knn,
                   svc_linear = model_svclinear,
                   svc_raidal = model_svcradial, 
                   lda = model_lda,
                   nb = model_nb)
resamples = resamples(model_list)
summary(resamples)
dotplot(resamples)
```
results show random foreast and SVM Radial worked the best among the used algorithms.




#NeuralNet
```{r}
#neuralnet from caret only deal with regression, whereas nnet deal with classification

#convert factor to numeric
train$Survived=as.numeric(as.character(train$Survived))
train$Pclass=as.numeric(as.character(train$Pclass))


model_nnet <- train(as.factor(Survived.1) ~.,  
                  method="nnet",
                  train_df, 
                  linout=FALSE, trace = FALSE,
                  preProcess = c("center", "scale"))
  
validate_df$pred <- predict(model_nnet, validate_df)
```

```{r nnet_err}
err <- mean(as.numeric(validate_df$pred ) != validate_df$Survived.1)
err
```

```{r impute_test}
test_partial = test[setdiff(colnames(validate_df),"Survived")]
test_partial$Pclass = as.numeric(as.character(test_partial$Pclass))
test_dmy <- dummyVars("~.", data=test_partial, fullRank=T) #generate n-1
test_transform <- data.frame(predict(test_dmy, newdata = test_partial))

```



#autoML
```{r auto, warning = FALSE, error = FALSE, message=FALSE}
train_df = as.h2o(train_df)
validate_df = as.h2o(validate_df) 

x = setdiff(colnames(train_df), "Survived.1")
y="Survived.1"

#autoML will metric error base on type of problem: regression or classification
train_df[,y] <- as.factor(train_df[,y])
validate_df[,y] <- as.factor(validate_df[,y])

aml <- h2o.automl(x = x, 
                  y = y,
                  training_frame = train_df,
                  leaderboard_frame = validate_df,
                  max_runtime_secs = 30)

# View the AutoML Leaderboard
lb <- aml@leaderboard
lb

```
```{r autopredict, warning = FALSE, error = FALSE, message=FALSE}
test_h2o <- as.h2o(test_transform)
h2o_pred <- h2o.predict(aml, test_h2o)
#0.775
```
Somehow autoML prediction didn't beat RF


#xgboost

```{r}
model_xgb <- xgboost(data = as.matrix(train_df[,2:9]), 
            label = train_df$Survived,
            nrounds = 100,
            objective = "binary:logistic",
            eta = 0.3,
            max_depth = 6,
            verbose = 0    # silent
)

validate_df$pred <- predict(model_xgb, as.matrix(validate_df))
#Error: 'predict' is not an exported object from 'namespace:xgboost'
```

```{r xgbfeatureimportance}
importance <- xgb.importance(colnames(train_df[,2:9]), model = model_xgb)
xgb.ggplot.importance(importance)
```
```{r xgbtree, fig.width=20}
xgb.plot.tree(feature_names = colnames(train_df[,2:9]), model = model_xgb, n_first_tree = 2)
```


```{r}
err <- mean(as.numeric(validate_df$pred > 0.5) != validate_df$Survived.1)
err
```

```{r}
#ROCR prediction function collide with other package, use ROCR::prediction
#prediction function doesn't take factor well
#pROC is need for multi-class ROC
library(ROCR)
predictions = predict(model_rf, validate_df)
pred <- ROCR::prediction(as.numeric(predictions), as.numeric(validate_df$Survived))
class(pred)
roc.perf = performance(pred, measure = "tpr", x.measure = "fpr")
plot(roc.perf)
abline(a=0, b= 1)

auc.perf = performance(pred, measure = "auc")
mtext(paste0("AUC=", round(auc.perf@y.values[[1]],2)) )

```

#Predict

```{r}
#submission <- predict(model_rf, test)
pred=ifelse(test$pred>=0.5,1,0)
test <- read_csv("test.csv")
submission = data.frame(test$PassengerId, pred)
colnames(submission) = c("PassengerId", "Survived")

write.csv(submission, "titanic_submission7.csv", row.names = FALSE)
```


```{r session}
sessionInfo()
#v5 rf
#v6 nnet
```

